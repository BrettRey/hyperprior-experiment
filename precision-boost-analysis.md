# Bridging Attention-Bias and Inductive Bias Distillation: Precision Boost for Recursive Competence

## Executive Summary

This document outlines a computational framework for replacing McCoy & Griffiths' complex MAML-based inductive bias distillation with a simpler "gain scalar k" approach inspired by the attention-bias hypothesis. The core insight is that a precision-weighting mechanism—analogous to the hyperprior proposed by Reynolds (2026)—can achieve equivalent recursive generalization while drastically reducing computational overhead.

---

## 1. Theoretical Foundation

### 1.1 The Attention-Bias Hypothesis (Reynolds, 2026)

The attention-bias hypothesis proposes that human language emergence is a **spandrel** driven by an inherited **attentional hyperprior** that amplifies precision-weighting on ostensive, hierarchically rhythmic signals. This hyperprior acts as a "neural volume knob" that boosts the precision of prediction errors generated by specific communicative cues.

**Key Formalization:**

In predictive-processing terms, the hyperprior modifies the likelihood function:

$$P(\text{data}|\theta) \propto \exp(-k \cdot \text{prediction error})$$

Where:
- $k \approx 1.5$ for ostensive, hierarchically rhythmic signals (human-like)
- $k = 1.0$ otherwise (baseline)

This scalar shift is sufficient to steer domain-general learning mechanisms toward linguistic structure.

### 1.2 The Steering vs. Learning Subsystem Architecture

Following Byrnes (2025), we adopt a two-subsystem architecture:

- **Steering Subsystem** (∼10% brain volume): Hypothalamus, brainstem—contains genetically specified cost functions, reward signals, and attentional priors
- **Learning Subsystem** (∼90% brain volume): Cortex, hippocampus, amygdala, cerebellum—domain-general predictive engine that learns from scratch

The attention-bias hypothesis places the hyperprior in the **steering subsystem**: a compact genetic specification that modulates what the learning subsystem pays attention to, without requiring specialized cortical modules.

---

## 2. The "Plus" Primitive: Hierarchical Rhythm Criteria ($P_t$)

### 2.1 Defining $P_t$ (Hierarchical Rhythm Predicate)

The hierarchical rhythm criteria $P_t$ identifies tokens that satisfy the ostensive, hierarchically rhythmic conditions that trigger the precision boost. Following CGEL (2025) and the Cambridge Grammar framework:

**$P_t(token)$ returns TRUE when:**

1. **Ostensive Cues Present:**
   - Token occurs in infant-directed speech register
   - Token follows/dis preceded by eye contact markers (gaze direction, face presence)
   - Token has contingent reactivity (rapid response to prior utterance)

2. **Hierarchical Rhythm Signatures:**
   - Token falls within syllable-sized chunks (50-300ms envelope)
   - Token participates in prosodic phrasing (stress-foot structure)
   - Token shows transitional probability patterns consistent with word boundaries

3. **Syntactic Bootstrapping (not semantic):**
   - Token occupies a position in a phase-complete structure
   - Token marks a phase boundary (complementizer, determiner, inflection)

### 2.2 The "Plus" Primitive in Context

In McCoy & Griffiths' framework, the "plus" primitive refers to a **recursive composition operator** that enables:
- Unbounded hierarchical embedding
- Systematic generalization beyond training distribution
- Compositional recursivity

The attention-bias account predicts that recursive competence emerges not from an explicit recursive operator, but from **precision-weighted attention** that amplifies the relevant structural cues during learning.

---

## 3. Computational Implementation: Gain Scalar $k$

### 3.1 Architecture Overview

We replace the MAML-based meta-learning distillation with a simple precision-modulated loss function:

```
Input: Token sequence w_1, w_2, ..., w_n
For each token w_i:
    if P_t(w_i) == TRUE:
        loss_i = base_loss_i * k  # k > 1 (precision boost)
    else:
        loss_i = base_loss_i       # k = 1 (baseline)
    
    accumulate gradients
```

### 3.2 Hierarchical Rhythm Detection ($P_t$ Implementation)

```python
def compute_pt(token, context, prosodic_features):
    """
    Returns True if token satisfies hierarchical rhythm criteria.
    
    Args:
        token: Current token
        context: Surrounding tokens and prosodic information
        prosodic_features: Dict with stress, duration, f0 features
    """
    # Criterion 1: Ostensive context detection
    is_ostensive = detect_ostensive_cues(context)
    
    # Criterion 2: Hierarchical rhythm detection
    has_rhythm = (
        prosodic_features['duration_ms'] between (50, 300) and
        prosodic_features['stress_pattern'] matches_pattern and
        compute_transitional_probability(token, context) > threshold
    )
    
    # Criterion 3: Syntactic phase completion (CGEL-compliant)
    is_phase_boundary = check_phase_boundary(token, context)
    
    return is_ostensive or (has_rhythm and is_phase_boundary)
```

### 3.3 Precision Boost Application

```python
def forward_with_precision_boost(model, input_ids, attention_mask, 
                                  prosodic_features, k=1.5):
    """
    Forward pass with precision-weighted loss.
    
    The gain scalar k amplifies prediction errors for tokens 
    satisfying P_t, simulating the attentional hyperprior.
    """
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    logits = outputs.logits
    
    # Compute base loss
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
    
    # Apply precision boost based on P_t
    precision_weights = torch.ones_like(loss)
    for i, token_id in enumerate(input_ids[0]):
        token = model.tokenizer.decode(token_id)
        if compute_pt(token, context, prosodic_features):
            precision_weights[i] = k  # Amplify error signal
    
    # Weighted loss simulates attentional gain
    weighted_loss = (loss * precision_weights).mean()
    
    return weighted_loss, precision_weights
```

---

## 4. Achieving Recursive Competence

### 4.1 Why $k > 1$ Enables Recursion

McCoy & Griffiths found that meta-learning (MAML) distills Bayesian priors into neural weights, enabling rapid generalization to recursive structures. The attention-bias hypothesis provides a **simpler mechanism**:

1. **Amplified Prediction Errors:** When $k > 1$, prediction errors from structurally-relevant tokens (those satisfying $P_t$) are amplified during training.

2. **Faster Syntactic Abstraction:** The model learns syntactic patterns more rapidly because errors in hierarchical positions (e.g., subject position, complementizer position) carry higher weight.

3. **Implicit Bayesian Prior:** The precision boost implicitly encodes a prior that "hierarchical structure matters"—without explicitly representing that prior in the weight initialization.

### 4.2 Formal Analysis

Let $E_i$ be the prediction error for token $i$:

- Without boost: $E_i^{\text{raw}} = \text{prediction} - \text{target}$
- With boost: $E_i^{\text{boosted}} = k \cdot E_i^{\text{raw}}$ for $P_t(i) =$ TRUE

The gradient update becomes:

$$\Delta \theta = -\alpha \cdot \sum_i \frac{\partial E_i^{\text{boosted}}}{\partial \theta}$$

This amplifies gradients from $P_t$-positive tokens by factor $k$, causing faster learning of patterns correlated with hierarchical rhythm cues.

### 4.3 Recursive Generalization Probes

Following McCoy & Griffiths, we test recursive competence with:

1. **Center-embedding tasks:** "The rat the cat chased died"
2. **Cross-serial dependencies:** "The farmers who the boys who the girls liked liked liked apples"
3. **Novel composition:** Structures not seen during training but compositional

**Prediction:** Models with $k > 1$ will achieve comparable or superior recursive generalization to MAML-trained models, with faster convergence and better sample efficiency (<100 examples).

---

## 5. Experimental Design

### 5.1 Baseline Comparisons

| Condition | Precision Boost $k$ | Description |
|-----------|---------------------|-------------|
| Control   | $k = 1.0$           | No attention bias (null model) |
| Human-like| $k = 1.5$           | Reynolds (2026) estimate |
| Strong    | $k = 2.0$           | Counterfactual high bias |
| MAML      | N/A                 | McCoy & Griffiths baseline |

### 5.2 Data Efficiency Requirement

Per cite [1780]: Prioritize learning from <100 examples.

**Implementation:**
- Training set: 50-100 sentences with recursive structures
- Test set: Novel recursive structures not in training
- Metric: Generalization accuracy to unseen recursive depth

### 5.3 Metrics

1. **Word-segmentation F1:** Does the model discover word boundaries?
2. **Subject-verb agreement accuracy:** Does the model learn grammatical dependencies?
3. **Hierarchical generalization probes:** Can the model handle deeper embedding than seen in training?

---

## 6. Linguistic Considerations (CGEL Framework)

### 6.1 Determinatives vs. DPs

Following CGEL (2025) and rejecting the DP hypothesis:
- **"The"** is treated as a **determinative** (D), not as the head of a DP
- The precision boost applies to determinatives in phase-complete positions
- This affects how we define $P_t$ for articles

### 6.2 Syntax-Semantics Distinction

The attention-bias mechanism operates on **syntactic** cues (hierarchical rhythm, phase boundaries), not semantic content:

- $P_t$ detects **form** (prosody, position, transitional probability)
- Semantic interpretation emerges later in processing
- The gain scalar steers syntactic learning specifically

This aligns with the operating principle to keep semantics and syntax conceptually distinct.

---

## 7. Predictions and Implications

### 7.1 Core Prediction

> A simple precision scalar ($k > 1$) applied to tokens satisfying hierarchical rhythm criteria ($P_t$) is sufficient to achieve recursive competence comparable to MAML-based inductive bias distillation.

### 7.2 Why This Matters

1. **Theoretical:** Provides a computationally simpler mechanism for the "biological preparedness" that McCoy & Griffiths implicitly encode via meta-learning.

2. **Empirical:** Makes testable predictions about neonatal attention (the fNIRS experiment in Reynolds, 2026).

3. **Evolutionary:** Connects to the spandrel account—recursive competence emerges from domain-general hardware when appropriately steered by inherited attention biases.

### 7.3 Comparison to MAML

| Aspect | MAML (McCoy & Griffiths) | Gain Scalar $k$ |
|--------|--------------------------|-----------------|
| Computational cost | High (meta-learning) | Low (single scalar) |
| Sample efficiency | High | High (via precision boost) |
| Interpretability | Implicit prior in weights | Explicit attention parameter |
| Biological plausibility | Unclear | High (matches predictive processing) |
| Recursive competence | Achieved | Predicted to achieve |

---

## 8. Implementation Roadmap

### Phase 1: Prototype
- [ ] Implement $P_t$ detector with prosodic features
- [ ] Create baseline LSTM/Transformer with $k$ scalar
- [ ] Test on simple recursive structures

### Phase 2: Evaluation
- [ ] Compare $k=1.0$, $k=1.5$, $k=2.0$ conditions
- [ ] Run hierarchical generalization probes
- [ ] Benchmark against MAML baseline

### Phase 3: Refinement
- [ ] Tune $k$ for optimal sample efficiency
- [ ] Explore ostensive cue detection
- [ ] Test with CGEL-compliant parsing

---

## 9. References

- Reynolds, B. (2026). *Noticing Language: Attention, Spandrels, and the Evolution of Linguistic Recursion* [Manuscript in preparation].
- McCoy, R.T., & Griffiths, T.L. (2025). Inductive bias distillation and recursive competence [Citation placeholder].
- Byrnes, J. (2025). Brain-Like AGI. [Citation in main.tex].
- Huddleston, R., & Pullum, G.K. (2002/2025). *The Cambridge Grammar of the English Language* (CGEL).
- Arnon, I., et al. (2025). Biocultural evolution of language. *Science*.

---

*Document generated for the Hyperprior Experiment, bridging Reynolds (2026) attention-bias framework with McCoy & Griffiths (2025) inductive bias distillation.*
